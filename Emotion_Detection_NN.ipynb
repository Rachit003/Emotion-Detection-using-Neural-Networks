{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Resizing, Rescaling, InputLayer, Conv2D, MaxPool2D, Activation, Add, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Dense, RandomRotation, RandomFlip, RandomContrast\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = \"C:\\ML\\dataset\\Emotions Dataset\\Emotions Dataset\\Train\"\n",
    "validation_directory = \"C:\\ML\\dataset\\Emotions Dataset\\Emotions Dataset\\Test\"\n",
    "class_names = [\"angry\", \"happy\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=class_names,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    validation_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=class_names,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_layers = tf.keras.Sequential([\n",
    "    RandomRotation(factor = (-0.025, 0.025)),\n",
    "    RandomFlip(mode = 'horizontal'),\n",
    "    RandomContrast(factor = 0.1)\n",
    "])\n",
    "\n",
    "def augmentation(image, label):\n",
    "    return augment_layers(image, training = True), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(augmentation, num_parallel_calls = tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(augmentation, num_parallel_calls = tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION = {\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"IMAGE_SIZE\" : 256,\n",
    "    \"LEARNING_RATE\" : 0.001,\n",
    "    \"DROPOUT_RATE\" : 0,\n",
    "    \"REGULARIZATION_RATE\" : 0,\n",
    "    \"EPOCHS\" : 25,\n",
    "    \"N_FILTERS\" : 6,\n",
    "    \"KERNEL_SIZE\" : 3,\n",
    "    \"N_STRIDES\" : 1,\n",
    "    \"POOL_SIZE\" : 2,\n",
    "    \"N_DENSE_1\" : 1024,\n",
    "    \"N_DENSE_2\" : 128,\n",
    "    \"NUM_CLASSES\" : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet_model(train_dataset, validation_dataset, CONFIGURATION):\n",
    "    resize_rescale = tf.keras.Sequential([\n",
    "        Resizing(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "        Rescaling(1/255)\n",
    "    ])\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        InputLayer(input_shape = (None, None, 3)),\n",
    "        resize_rescale,\n",
    "\n",
    "        Conv2D(filters = CONFIGURATION[\"N_FILTERS\"], kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides = CONFIGURATION[\"N_STRIDES\"], \n",
    "            activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size = CONFIGURATION[\"POOL_SIZE\"], strides = CONFIGURATION[\"N_STRIDES\"]*2),\n",
    "        Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"]),\n",
    "\n",
    "        Conv2D(filters = CONFIGURATION[\"N_FILTERS\"]*2 + 4, kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides = CONFIGURATION[\"N_STRIDES\"], \n",
    "            activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size = CONFIGURATION[\"POOL_SIZE\"], strides = CONFIGURATION[\"N_STRIDES\"]*2),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(CONFIGURATION[\"N_DENSE_1\"], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"]),\n",
    "\n",
    "        Dense(CONFIGURATION[\"N_DENSE_2\"], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\")\n",
    "    ])\n",
    "\n",
    "    loss_function = CategoricalCrossentropy()\n",
    "    metrics = [CategoricalAccuracy(name = \"Accuracy\"), TopKCategoricalAccuracy(k = 2, name = \"Top_K_Accuracy\")]\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]),\n",
    "        loss = loss_function,\n",
    "        metrics = metrics\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data = validation_dataset,\n",
    "        epochs = CONFIGURATION[\"EPOCHS\"],\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = lenet_model(train_dataset, validation_dataset, CONFIGURATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(model, validation_dataset, class_names):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    for im, lb in validation_dataset:\n",
    "        for i in range(32):\n",
    "\n",
    "            test_image = tf.expand_dims(im[i], axis = 0)\n",
    "            pred = class_names[tf.argmax(model(test_image).numpy()[0])]\n",
    "            predictions = np.append(predictions, pred)\n",
    "\n",
    "            labels = np.append(labels, class_names[tf.argmax(lb[i].numpy())])\n",
    "            if predictions.shape[0] == 2278:\n",
    "                break\n",
    "\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(predictions, labels):\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.heatmap(cm,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = predicting(model, validation_dataset, class_names)\n",
    "plot_cm(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv2D(Layer):\n",
    "    def __init__(self, n_filters, kernel_size, n_strides, padding = \"same\"):\n",
    "        super(CustomConv2D, self).__init__(name = \"Custom_Conv_Layer\")\n",
    "\n",
    "        self.conv = Conv2D(filters = n_filters,kernel_size = kernel_size, activation = 'relu', \n",
    "                           strides = n_strides, padding = padding)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "\n",
    "    def call(self, input, training = True):\n",
    "        x = self.conv(input)\n",
    "        x = self.batchnorm(x, training = True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_channels, n_strides = 1):\n",
    "        super(ResidualBlock, self).__init__(name = \"Residual_block\")\n",
    "\n",
    "        self.dotted = (n_strides!=1)\n",
    "\n",
    "        self.custom_conv_1 = CustomConv2D(n_channels, 3, n_strides, padding = \"same\")\n",
    "        self.custom_conv_2 = CustomConv2D(n_channels, 3, 1, padding = \"same\")\n",
    "        self.activation = Activation(\"relu\")\n",
    "\n",
    "        if self.dotted:\n",
    "            self.custom_conv_3 = CustomConv2D(n_channels, 1, n_strides)\n",
    "\n",
    "    def call(self, input, training = True):\n",
    "        x = self.custom_conv_1(input, training = True)\n",
    "        x = self.custom_conv_2(x, training = True)\n",
    "\n",
    "        if self.dotted:\n",
    "            x_add = self.custom_conv_3(input, training = True)\n",
    "            x_add = Add()([x,x_add])\n",
    "        else:\n",
    "            x_add = Add()([x,input])\n",
    "        \n",
    "        return self.activation(x_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet_Model(Model):\n",
    "    def __init__(self):\n",
    "        super(Resnet_Model, self).__init__(name = \"resnet34\")\n",
    "\n",
    "        self.conv_1 = CustomConv2D(64, 7, 2)\n",
    "        self.maxpool = MaxPool2D(3, 2)\n",
    "\n",
    "        self.conv_2_1 = ResidualBlock(64)\n",
    "        self.conv_2_2 = ResidualBlock(64)\n",
    "        self.conv_2_3 = ResidualBlock(64)\n",
    "\n",
    "        self.conv_3_1 = ResidualBlock(128, 2)\n",
    "        self.conv_3_2 = ResidualBlock(128)\n",
    "        self.conv_3_3 = ResidualBlock(128)\n",
    "        self.conv_3_4 = ResidualBlock(128)\n",
    "\n",
    "        self.conv_4_1 = ResidualBlock(256, 2)\n",
    "        self.conv_4_2 = ResidualBlock(256)\n",
    "        self.conv_4_3 = ResidualBlock(256)\n",
    "        self.conv_4_4 = ResidualBlock(256)\n",
    "        self.conv_4_5 = ResidualBlock(256)\n",
    "        self.conv_4_6 = ResidualBlock(256)\n",
    "\n",
    "        self.conv_5_1 = ResidualBlock(512, 2)\n",
    "        self.conv_5_2 = ResidualBlock(512)\n",
    "        self.conv_5_3 = ResidualBlock(512)\n",
    "\n",
    "        self.global_pool = GlobalAveragePooling2D()\n",
    "        self.fc_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\")\n",
    "\n",
    "    def call(self, x, training = True):\n",
    "        x = self.conv_1(x, training = True)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv_2_1(x, training = True)\n",
    "        x = self.conv_2_2(x, training = True)\n",
    "        x = self.conv_2_3(x, training = True)\n",
    "\n",
    "        x = self.conv_3_1(x, training = True)\n",
    "        x = self.conv_3_2(x, training = True)\n",
    "        x = self.conv_3_3(x, training = True)\n",
    "        x = self.conv_3_4(x, training = True)\n",
    "\n",
    "        x = self.conv_4_1(x, training = True)\n",
    "        x = self.conv_4_2(x, training = True)\n",
    "        x = self.conv_4_3(x, training = True)\n",
    "        x = self.conv_4_4(x, training = True)\n",
    "        x = self.conv_4_5(x, training = True)\n",
    "        x = self.conv_4_6(x, training = True)\n",
    "\n",
    "        x = self.conv_5_1(x, training = True)\n",
    "        x = self.conv_5_2(x, training = True)\n",
    "        x = self.conv_5_3(x, training = True)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        return self.fc_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_model_run(train_dataset, validation_dataset, CONFIGURATION):\n",
    "    resnet = Resnet_Model()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        'best_weights.keras',\n",
    "        monitor = 'val_accuracy',\n",
    "        mode = 'max',\n",
    "        verbose = 1,\n",
    "        save_best_only = True\n",
    "    )\n",
    "\n",
    "    loss_function = CategoricalCrossentropy()\n",
    "    metrics = [CategoricalAccuracy(name = \"Accuracy\"), TopKCategoricalAccuracy(k = 2, name = \"Top_K_Accuracy\")]\n",
    "\n",
    "    resnet.compile(\n",
    "            optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]*10),\n",
    "            loss = loss_function,\n",
    "            metrics = metrics\n",
    "        )\n",
    "\n",
    "    history = resnet.fit(\n",
    "            train_dataset,\n",
    "            validation_data = validation_dataset,\n",
    "            epochs = 1,\n",
    "            # epochs = CONFIGURATION[\"EPOCHS\"],\n",
    "            verbose = 1,\n",
    "            callbacks = [checkpoint_callback]\n",
    "        )\n",
    "    \n",
    "    return resnet, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet, history = resnet_model_run(train_dataset.take(1), validation_dataset, CONFIGURATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(Model):\n",
    "    def __init__(self):\n",
    "        super(EfficientNet, self).__init__(name = \"EfficientNet\")\n",
    "\n",
    "        self.backbone = tf.keras.applications.EfficientNetB4(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"], 3),\n",
    "        )\n",
    "        self.backbone.trainable = False\n",
    "\n",
    "        self.efficientnet = tf.keras.Sequential([\n",
    "            InputLayer(shape=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"], 3)),\n",
    "            self.backbone,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\"),\n",
    "            Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.efficientnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnet_model_run(train_dataset, validation_dataset, CONFIGURATION):\n",
    "    efficientnet = EfficientNet()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        'best_weights.keras',\n",
    "        monitor = 'val_accuracy',\n",
    "        mode = 'max',\n",
    "        verbose = 1,\n",
    "        save_best_only = True\n",
    "    )\n",
    "\n",
    "    loss_function = CategoricalCrossentropy()\n",
    "    metrics = [CategoricalAccuracy(name = \"Accuracy\"), TopKCategoricalAccuracy(k = 2, name = \"Top_K_Accuracy\")]\n",
    "\n",
    "    efficientnet.compile(\n",
    "            optimizer = Adam(learning_rate = CONFIGURATION[\"LEARNING_RATE\"]*10),\n",
    "            loss = loss_function,\n",
    "            metrics = metrics\n",
    "        )\n",
    "    \n",
    "    history = efficientnet.fit(\n",
    "            train_dataset,\n",
    "            validation_data = validation_dataset,\n",
    "            epochs = 1,\n",
    "            # epochs = CONFIGURATION[\"EPOCHS\"],\n",
    "            verbose = 1,\n",
    "            callbacks = [checkpoint_callback]\n",
    "        )\n",
    "    \n",
    "    return efficientnet, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet, history = efficientnet_model_run(train_dataset, validation_dataset, CONFIGURATION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
